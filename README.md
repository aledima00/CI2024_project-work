# CI2024_project-work
Project Work for the exam of Computational Intelligence Course of 2024/2025 @ PoliTo

## Overview

This project focuses on Symbolic Regression using Genetic Programming (GP) in Python. The goal is to evolve mathematical expressions that best fit given data, including all the functions (or most of them) from the `numpy` library.

The project has been carried out using the library `symgp`, that i developed on my own as an independent framework for symbolic regression based on genetic programming.

The official repo can be found on my github (https://github.com/aledima00/symgp), but in this case i pasted the repo as a sub-folder of this project (git submodule was giving issues due to the lack of support in *poetry* of relative paths when adding dependencies).

## Data

To test the framework, i used the data provided by the professor in the github repo of the course: (https://github.com/squillero/computational-intelligence/tree/master/2024-25/project-work/data).
In particular, i provide outputs related to the problems 1 to 8.

## Methodology

1. **Loading Data**: The data are loaded from the `.npz` files using NumPy APIs.
2. **Defining Input Variables**: Input variable names are defined based on the shape of the input data, allowing non-fixed input size. Note that the library accepts several inputs, so np arrays are split in sub-arrays corresponding to different inputs of the functions.
3. **Setting Generation Parameters**: Various parameters for the generation process are set, including the use of integer constants, mean and standard deviation of random constants, and ratios of different types of constants and operators.
4. **Initializing the Model**: The `BaseModel` from the `symgp` library is initialized with specified parameters such as maximum depth, population size, input variable names, random seed, and generation parameters. Note that the `BaseModel` is the class wrapping `Model` providing as *Functional Set* the whole suite of predefined numpy functions wrapped by the operators in the `npf` submoudle.
5. **Populating Initial Population**: The initial population is randomly generated using the `populate()` method of the `BaseModel`.
6. **Evolving the Population**: The population is evolved over a specified number of generations using the `evolve()` method of the `BaseModel`. Parameters such as mutation rate, elitism rate, pool size, parsimony weight, and parsimony format are provided.
7. **Evaluating the Best Individual**: The best individual from the evolved population is evaluated for its fitness against the provided data, including in the measure both MSE and parsimony pressure, in a proportion defined by the user.

## Results

The results of the symbolic regression for each problem are provided in the `s314935.py` file. The estimated functions and their mean squared error (MSE) values are as follows:

- **Problem 1**:
  - Estimated Function: `np.sin(x[0])`
  - MSE: `0.000e+00`

- **Problem 2**:
  - Estimated Function: `x[0]`
  - MSE: `2.962e+13`

- **Problem 3**:
  - Estimated Function: `np.multiply(x[1],np.subtract(x[1],np.multiply(x[1],x[1])))`
  - MSE: `5.166e+02`

- **Problem 4**:
  - Estimated Function: `2.0`
  - MSE: `2.162e+01`

- **Problem 5**:
  - Estimated Function: `0.0`
  - MSE: `5.573e-18`

- **Problem 6**:
  - Estimated Function: `np.add(np.minimum(np.subtract(2.0,x[0]),x[1]),np.minimum(x[1],x[1]))`
  - MSE: `1.339e+00`

- **Problem 7**:
  - Estimated Function: `9.0`
  - MSE: `7.127e+02`

- **Problem 8**:
  - Estimated Function: `x[5]`
  - MSE: `2.299e+07`

## Conclusion

The project successfully demonstrates the use of Genetic Programming for Symbolic Regression using the `symgp` library, even if for some functions the MSE does not seems to be very accurate.
Further work may reduce the MSE also for more complex functions, fine-tuning parameters and and the internal configuration of the `symgp` library.

Moreover, it can be noticed that many of the predicted functions are constants, which may indicate that the model found simple solutions that fit the data well: indeed, i tried several configurations and despite the strange behavior of returning a constant, the MSE was not increasing so much with respect to other cases, so the choice respected the parsimony pressure choosing a function very simple even if there is a sliht decrease in MSE.

Basically, the model choose simplicity over the slight MSE increase obtained very complex functions generated by other configurations.

In conclusion, the experiment highlights the potential of GP in evolving mathematical expressions, but further work can be done, especially:
- more adjustments can be done to balance simplicity and accuracy
- some bad results shows that the model is lacking for some particular types of data, for which it may be useful to explore different techniques
- some work can be done also in preventing overfitting, as all the MSE evaluation has been done on the training data, but generalization may be an additional problem, especially when dealing with noisy data
